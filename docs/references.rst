References
==========


Papers
------

.. bibliography:: refs.bib
    :style: plain


Datasets
--------------------------

Our work is most closely related to :cite:`Lefortier2016`.
:cite:`Lefortier2016` introduces a large-scale logged bandit feedback data (`Criteo dataset <http://www.cs.cornell.edu/~adith/Criteo/>`_.) from a leading company in the display advertising, Criteo.
The data contains context vectors of user impressions, advertisements (ads) as actions, and click indicators as reward.
It also provides the ex ante probability of each ad being selected by the behavior policy.
Therefore, this data can be used to compare different *off-policy learning methods*, which aim to learn a new bandit policy using only log data generated by a behavior policy.
However, the Criteo data has limitations, which we overcome as follows:

* The Criteo dataset does not provide the code (production implementation) of their behavior policy. Moreover, the data was collected by running only a single behavior policy. As a result, this data cannot be used for evaluation and comparison of different OPE estimators.

:math:`\rightarrow` In contrast, we provide the code of our behavior policies (i.e., Bernoulli TS and Random), which allows researchers to re-run the same behavior policies on the log data. Our open data also contains logged bandit feedback data generated by \textit{multiple} behavior policies. It enables the evaluation and comparison of different OPE estimators, as we demonstrated in Section~\ref{sec:off-policy_estimator_selection}. This is the first large-scale bandit dataset that enables such evaluation of OPE with the ground-truth policy value of behavior policies.

*  :cite:`Lefortier2016` does not provide a pipeline implementation to handle their data. Researchers have to re-implement the experimental environment by themselves before implementing their own methods. This may lead to inconsistent experimental conditions across different studies, potentially causing reproducibility issues.

:math:`\rightarrow` We implement the Open Bandit Pipeline to simplify and standardize the experimental processing of bandit algorithms and OPE using our open data. This tool contributes to the reproducible and transparent use of our data.


Projects
----------

This project is strongly inspired by **Open Graph Benchmark** --a collection of benchmark datasets, data loaders, and evaluators for graph machine learning:
`[github] <https://github.com/snap-stanford/ogb>`_ `[project page] <https://ogb.stanford.edu>`_ `[paper] <https://arxiv.org/abs/2005.00687>`_.
