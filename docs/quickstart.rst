============
Quickstart
============

We show an example of conducting offline evaluation of the performance of Bernoulli Thompson Sampling (BernoulliTS) as a counterfactual policy using *Replay Method*
and logged bandit feedback generated by the Random policy (behavior policy).
We see that only ten lines of code are sufficient to complete OPE from scratch.

.. code-block:: python

    # a case for implementing OPE of the BernoulliTS policy using log data generated by the Random policy
    from obp.dataset import OpenBanditDataset
    from obp.policy import BernoulliTS
    from obp.simulator import run_bandit_simulation
    from obp.ope import OffPolicyEvaluation ReplayMethod

    # (1) Data loading and preprocessing
    dataset = OpenBanditDataset(behavior_policy='random', campaign='all')
    train, test = dataset.split_data(test_size=0.3, random_state=42)

    # (2) Offline Bandit Simulation
    counterfactual_policy = BernoulliTS(n_actions=dataset.n_actions, len_list=dataset.len_list)
    selected_actions = run_bandit_simulation(train=train, policy=counterfactual_policy)

    # (3) Off-Policy Evaluation
    ope = OffPolicyEvaluation(train=train, ope_estimators=[ReplayMethod()])
    estimated_policy_value = ope.estimate_policy_values(selected_actions=selected_actions)

    # estimated performance of BernoulliTS relative to the ground-truth performance of Random
    relative_policy_value_of_bernoulli_ts = estimated_policy_value['rm'] / test['reward'].mean()
    print(relative_policy_value_of_bernoulli_ts) # 1.21428...

A detailed introduction with the same example can be found at `quickstart <https://github.com/st-tech/zr-obp/blob/master/examples/quickstart/quickstart.ipynb>`_.
Below, we explain some important features in the example flow.


Data loading and preprocessing
------------------------------------

We prepare easy-to-use data loader for Open Bandit Dataset.
We handle dataset preprocessing as well as standardized train/test splitting.

.. code-block:: python

    # load and preprocess raw data in "Women" campaign collected by the Random policy
    dataset = OpenBanditDataset(behavior_policy='random', campaign='women')
    # Split the data into 70% training and 30% test sets
    train, test = dataset.split_data(test_size=0.3, random_state=0)

    print(train.keys())
    # dict_keys(['n_rounds', 'n_actions', 'action', 'position', 'reward', 'pscore', 'context', 'action_context'])

Users can implement their own feature engineering in the :class:`pre_process` method of :class:`obp.dataset.OpenBanditDataset` class.
We show an example of implementing some new feature engineering processes in `./examples/obd/dataset.py <https://github.com/st-tech/zr-obp/blob/master/examples/obd/dataset.py>`_.
Moreover, by following the interface of :class:`obp.dataset.BaseBanditDataset` class, one can handle future open datasets for bandit algorithms other than our OBD.

Offline Bandit Simulation
------------------------------

After preparing a dataset, we now run **offline bandit simulation** on the logged bandit feedback as follows.

.. code-block:: python

    # define a counterfacutal policy, which is the Bernoulli TS policy here
    counterfactual_policy = BernoulliTS(n_actions=dataset.n_actions, len_list=dataset.len_list)
    # run an offline bandit simulation on the training set
    # selected_actions is an array containing selected actions by counterfactual policy in an simulation
    selected_actions = run_bandit_simulation(train=train, policy=counterfactual_policy)

:class:`obp.simulator.run_bandit_simulation` function takes :class:`obp.policy.BanditPolicy` and :class:`train` (a dictionary storing the training set of the bandit feedback) as inputs and runs offline bandit simulation of a given bandit policy.
:class:`selected_actions` is an array of selected actions during the offline bandit simulation by the counterfactual policy.
Users can implement their own bandit algorithms by following the interface of :class:`obp.policy.BasePolicy`.


Off-Policy Evaluation
------------------------------

Our final step is **off-policy evaluation** (OPE), which attempts to estimate the performance of bandit algorithms using log data generated by offline bandit simulations.
Our pipeline also provides an easy procedure for doing OPE as follows.

.. code-block:: python

    # estimate the policy value of BernoulliTS based on actions selected by that policy
    # it is possible to set multiple OPE estimators to the `ope_estimators` argument
    ope = OffPolicyEvaluation(train=train, ope_estimators=[ReplayMethod()])
    estimated_policy_value = ope.estimate_policy_values(selected_actions=selected_actions)
    print(estimated_policy_value) # {'rm': 0.003717..}

    # comapre the estimated performance of BernoulliTS (counterfactual policy)
    # with the ground-truth performance of Random (behavior policy)
    relative_policy_value_of_bernoulli_ts = estimated_policy_value['rm'] / test['reward'].mean()
    # our OPE procedure suggests that BernoulliTS improves Random by 21.4%
    print(relative_policy_value_of_bernoulli_ts) # 1.21428...

Users can implement their own OPE estimator by following the interface of :class:`obp.ope.BaseOffPolicyEstimator` class.
:class:`obp.ope.OffPolicyEvaluation` class summarizes and compares the estimated policy values by several off-policy estimators.
A detailed usage of this class can be found at `quickstart <https://github.com/st-tech/zr-obp/tree/master/examples/quickstart>`_. :class:`test['reward'].mean()` is the empirical mean of factual rewards in the log and thus is the ground-truth performance of the behavior policy (the Random policy in this example.).
