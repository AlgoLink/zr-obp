============
Quickstart
============

We show an example of conducting an offline evaluation of the performance of BernoulliTS using *Inverse Probability Weighting* as an OPE estimator and the Random policy as a behavior policy.
We see that only ten lines of code are sufficient to complete OPE from scratch.

.. code-block:: python

    # a case for implementing OPE of the BernoulliTS policy using log data generated by the Random policy
    from obp.dataset import OpenBanditDataset
    from obp.policy import BernoulliTS
    from obp.simulator import OfflineBanditSimulator

    # (1) Data loading and preprocessing
    dataset = OpenBanditDataset(behavior_policy='random', campaign='women')
    train, test = dataset.split_data(test_size=0.3, random_state=42)

    # (2) Offline Bandit Simulation
    simulator = OfflineBanditSimulator(train=train)
    counterfactual_policy = BernoulliTS(
    n_actions=dataset.n_actions,
    len_list=dataset.len_list,
    random_state=42)
    simulator.simulate(policy=counterfactual_policy)

    # (3) Off-Policy Evaluation
    estimated_policy_value = simulator.inverse_probability_weighting()

    # estimated performance of BernoulliTS relative to the ground-truth performance of Random
    relative_policy_value_of_bernoulli_ts = estimated_policy_value / test['reward'].mean()
    print(relative_policy_value_of_bernoulli_ts) # 1.21428...


A gentle introduction with the same example can be found at `quickstart <https://github.com/st-tech/zr-obp/blob/master/examples/quickstart/quickstart.ipynb>`_.
Below, we explain some important features in the example flow.


Data loading and preprocessing
------------------------------------

We prepare easy-to-use data loader for Open Bandit Dataset.
We handle dataset preprocessing as well as standardized train/test splitting.

.. code-block:: python

    # load and preprocess raw data in "Women" campaign collected by the Random policy
    dataset = OpenBanditDataset(behavior_policy='random', campaign='women')
    # Split the data into 70% training and 30% test sets
    train, test = dataset.split_data(test_size=0.3, random_state=0)

    print(train.keys())
    # dict_keys(['n_data', 'n_actions', 'action', 'position', 'reward', 'pscore', 'X_policy', 'X_reg', 'X_user'])

Users can implement their own feature engineering in the :class:`pre_process` method of :class:`OpenBanditDataset` class.
We show an example of implementing some new feature engineering processes in `./examples/obd/dataset.py <https://github.com/st-tech/zr-obp/blob/master/examples/obd/dataset.py>`_.

Moreover, by following the interface of `BaseBanditDataset` class in `./obp/dataset.py <https://github.com/st-tech/zr-obp/blob/master/obp/dataset.py>`_, one can handle future open datasets for bandit algorithms other than our OBD.

Offline Bandit Simulation
------------------------------

After preparing a dataset, we now run an **offline bandit simulation** on the logged bandit feedback as follows.

.. code-block:: python

    # define a simulator object
    simulator = OfflineBanditSimulator(train=train)
    # define a counterfacutal policy, which is the Bernoulli TS policy here
    counterfactual_policy = BernoulliTS(
    n_actions=dataset.n_actions,
    len_list=dataset.len_list,
    random_state=42)
    # run an offline bandit simulation on the training set
    simulator.simulate(policy=counterfactual_policy)


The simulation takes :class:`BanditPolicy` class and :class:`train` (a dictionary storing the training set of the bandit feedback) as inputs and runs offline bandit simulation of a given bandit policy.
Users can implement their own bandit algorithms by following the interface of :class:`BaseContextFreePolicy` in `./obp/policy/contextfree.py <https://github.com/st-tech/zr-obp/blob/master/obp/policy/contextfree.py>`_ or :class:`BaseContexttualPolicy` in `./obp/policy/contextual.py <https://github.com/st-tech/zr-obp/blob/master/obp/policy/contextual.py>`_.


Off-Policy Evaluation
------------------------------

Our final step is **off-policy evaluation** (OPE), which attempts to estimate the performance of bandit algorithms using log data generated by offline bandit simulations.
Our pipeline also provides an easy procedure for doing OPE as follows.

.. code-block:: python

    # estimate the policy value of BernoulliTS based on actions selected by that policy
    estimated_policy_value = simulator.inverse_probability_weighting()

    # comapre the estimated performance of BernoulliTS (counterfactual policy)
    # with the ground-truth performance of Random (behavior policy)
    relative_policy_value_of_bernoulli_ts = estimated_policy_value / test['reward'].mean()
    # Our OPE procedure estimates that BernoulliTS improves Random by 21.4%
    print(relative_policy_value_of_bernoulli_ts) # 1.21428...

Users can implement their own OPE estimator as a method of :class:`OfflineBanditSimator` class.
:class:`test['reward'].mean()` is the empirical mean of factual rewards in the log and thus is the ground-truth performance of the behavior policy (the Random policy in this example.).
