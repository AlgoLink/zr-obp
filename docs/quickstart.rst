============
Quickstart
============

We show an example of conducting offline evaluation of the performance of Bernoulli Thompson Sampling (BernoulliTS) as a counterfactual policy using *Replay Method*
and logged bandit feedback generated by the Random policy (behavior policy).
We see that only ten lines of code are sufficient to complete OPE from scratch.

.. code-block:: python

    # a case for implementing OPE of the BernoulliTS policy using log data generated by the Random policy
    from obp.dataset import OpenBanditDataset
    from obp.policy import BernoulliTS
    from obp.simulator import run_bandit_simulation
    from obp.ope import OffPolicyEvaluation, ReplayMethod

    # (1) Data loading and preprocessing
    dataset = OpenBanditDataset(behavior_policy='random', campaign='women')
    bandit_feedback = dataset.obtain_batch_bandit_feedback()

    # (2) Offline Bandit Simulation
    counterfactual_policy = BernoulliTS(n_actions=dataset.n_actions, len_list=dataset.len_list)
    selected_actions = run_bandit_simulation(bandit_feedback=bandit_feedback, policy=counterfactual_policy)

    # (3) Off-Policy Evaluation
    ope = OffPolicyEvaluation(bandit_feedback=bandit_feedback, ope_estimators=[ReplayMethod()])
    estimated_policy_value = ope.estimate_policy_values(selected_actions=selected_actions)

    # estimated performance of BernoulliTS relative to the ground-truth performance of Random
    relative_policy_value_of_bernoulli_ts = estimated_policy_value['rm'] / bandit_feedback['reward'].mean()
    print(relative_policy_value_of_bernoulli_ts) # 1.120574...

A detailed introduction with the same example can be found at `quickstart <https://github.com/st-tech/zr-obp/blob/master/examples/quickstart/quickstart.ipynb>`_.
Below, we explain some important features in the example flow.


Data loading and preprocessing
------------------------------------

We prepare easy-to-use data loader for Open Bandit Dataset.

.. code-block:: python

    # load and preprocess raw data in "Women" campaign collected by the Random policy
    dataset = OpenBanditDataset(behavior_policy='random', campaign='women')
    # obtain logged bandit feedback generated by behavior policy
    bandit_feedback = dataset.obtain_batch_bandit_feedback()

    print(bandit_feedback.keys())
    # dict_keys(['n_rounds', 'n_actions', 'action', 'position', 'reward', 'pscore', 'context', 'action_context'])

Users can implement their own feature engineering in the :class:`pre_process` method of :class:`obp.dataset.OpenBanditDataset` class.
We show an example of implementing some new feature engineering processes in `./examples/obd/dataset.py <https://github.com/st-tech/zr-obp/blob/master/examples/obd/dataset.py>`_.
Moreover, by following the interface of :class:`obp.dataset.BaseBanditDataset` class, one can handle their own or future open datasets for bandit algorithms other than our OBD.

Offline Bandit Simulation
------------------------------

After preparing a dataset, we now run **offline bandit simulation** on the logged bandit feedback as follows.

.. code-block:: python

    # define a counterfacutal policy (the Bernoulli TS policy here)
    counterfactual_policy = BernoulliTS(n_actions=dataset.n_actions, len_list=dataset.len_list)
    # `selected_actions` is an array containing selected actions by counterfactual policy in an simulation
    selected_actions = run_bandit_simulation(bandit_feedback=bandit_feedback, policy=counterfactual_policy)

:class:`obp.simulator.run_bandit_simulation` function takes :class:`obp.policy.BanditPolicy` and :class:`train` (a dictionary storing the logged bandit feedback) as inputs and runs offline bandit simulation of a given bandit policy.
:class:`selected_actions` is an array of selected actions during the offline bandit simulation by the counterfactual policy.
Users can implement their own bandit algorithms by following the interface of :class:`obp.policy.BasePolicy`.


Off-Policy Evaluation
------------------------------

Our final step is **off-policy evaluation** (OPE), which attempts to estimate the performance of bandit algorithms using log data generated by offline bandit simulation.
Our pipeline also provides an easy procedure for doing OPE as follows.

.. code-block:: python

    # estimate the policy value of BernoulliTS based on actions selected by that policy in offline bandit simulation
    # it is possible to set multiple OPE estimators to the `ope_estimators` argument
    ope = OffPolicyEvaluation(bandit_feedback=bandit_feedback, ope_estimators=[ReplayMethod()])
    estimated_policy_value = ope.estimate_policy_values(selected_actions=selected_actions)
    print(estimated_policy_value) # {'rm': 0.005155..} dictionary containing estimated policy values by each OPE estimator.

    # comapre the estimated performance of BernoulliTS (counterfactual policy)
    # with the ground-truth performance of Random (behavior policy)
    relative_policy_value_of_bernoulli_ts = estimated_policy_value['rm'] / bandit_feedback['reward'].mean()
    # our OPE procedure suggests that BernoulliTS improves Random by 12.05%
    print(relative_policy_value_of_bernoulli_ts) # 1.120574...

Users can implement their own OPE estimator by following the interface of :class:`obp.ope.BaseOffPolicyEstimator` class.
:class:`obp.ope.OffPolicyEvaluation` class summarizes and compares the estimated policy values by several off-policy estimators.
A detailed usage of this class can be found at `quickstart <https://github.com/st-tech/zr-obp/tree/master/examples/quickstart>`_.
:class:`bandit_feedback['reward'].mean()` is the empirical mean of factual rewards (on-policy estimate of the policy value) in the log and thus is the ground-truth performance of the behavior policy (the Random policy in this example.).
