================================================
Overview of Off-Policy Evaluation
================================================


Setup
------

We consider a general multi-armed contextual bandit setting.
Let :math:`{\cal A}=\{0,...,m\}` be a finite set of :math:`m+1` *actions* (equivalently, *arms* or *treatments*), that the decision maker can choose from.
Let :math:`Y(\cdot): {\cal A}\rightarrow \mathbb{R}` denote a potential reward function that maps actions into rewards or outcomes, where :math:`Y(a)` is the reward when action :math:`a` is chosen (e.g., whether a fashion item as an action results in a click).
Let :math:`X` denote *context* vector (e.g., the user's demographic profile and user-item interaction history) that the decision maker observes when picking an action.
We denote the finite set of possible contexts by :math:`{\cal X}`.
We think of :math:`(Y(\cdot),X)` as a random vector with unknown distribution :math:`G`.
Given a vector of :math:`(Y(\cdot),X)`, we define the mean reward function :math:`\mu: {\cal X} \times {\cal A} \rightarrow \mathbb{R}` as :math:`\mu(x, a) = \mathbb{E} [Y (a) | X=x ]`.

We call a function :math:`\pi: {\cal X} \rightarrow {\cal A}` a *policy*, which maps each context :math:`x \in {\cal X}` into a distribution over actions, where :math:`\pi (a | x)` is the probability of taking action :math:`a` given a context vector :math:`x`.
Let :math:`\{(Y_t,X_t,D_t)\}_{t=1}^T` be historical logged bandit feedback with :math:`T` rounds of observations.
:math:`D_t=(D_{t0},...,D_{tm})'` where :math:`D_{ta}` is a binary variable indicating whether action :math:`a` is chosen in round :math:`t`.
:math:`Y_t=\sum_{a=0}^{m}D_{ta}Y_t(a)` and :math:`X_t` denote the reward and the context observed in round :math:`t`, respectively.
We assume that a logged bandit feedback is generated by a \textit{behavior policy} `\pi_b` as follows:


* In each round :math:`t=1,...,T`, :math:`(Y_t(\cdot),X_t)` is i.i.d. drawn from distribution :math:`G`.
* Given :math:`X_t`, an action is randomly chosen based on :math:`\pi_b(\cdot|X_t)`, creating the action choice :math:`D_{t}` and the associated reward :math:`Y_t`.

Suppose that :math:`\pi_b` is fixed for all rounds, and thus :math:`D_t` is i.i.d. across rounds.
Because :math:`(Y_t(\cdot),X_t)` is i.i.d. across rounds and :math:`Y_t=\sum_{a=0}^m D_{ta}Y_t(a)`, each observation :math:`(Y_t,X_t,D_t)` is i.i.d. across rounds.
Note that :math:`D_t` is independent of :math:`Y_t(\cdot)` conditional on :math:`X_t`.


Examples
---------
Our setup allows for many popular multi-armed bandit algorithms, as the following examples illustrate.

Random A/B testing
~~~~~~~~~~~~~~~~~~~~
We always choose each action uniformly at random: :math:`\pi_b(\cdot|X) =\frac{1}{m+1}` always holds for any :math:`a \in {\cal A}` and :math:`X \in {\cal X}`.


Bernoulli Thompson Sampling
~~~~~~~~~~~~~~~~~~~~~~~~~~~~
When the context :math:`X_t` is given, we sample the potential reward :math:`\tilde{Y}(a)` from the beta distribution :math:`Beta (S_{ta} + \alpha, F_{ta} + \beta) ` for each action, where :math:`S_{ta} = \sum_{t'=1}^{t-1} Y_{t'}D_{t'a}, F_{ta} = (t-1) - S_{ta}`.
:math:`\alpha, \beta` are the parameters of the prior Beta distribution.
We then choose the action with the highest sampled potential reward, :math:`\argmax_{a^{\prime} \in {\cal A}}\tilde{Y}(a^{\prime})`.
As a result, this algorithm chooses actions with the following probabilities:

.. math::
    \pi(a|X_t) = \Pr\{a=\argmax_{a'\in {\cal A}}\tilde{Y}(a^{\prime})\}.


Prediction Target
-------------------
We are interested in using the historical logged bandit data to estimate the following *policy value* of any given *counterfactual policy* :math:`\pi` which might be different from :math:`\pi_b`:

.. math::
    V^{\pi} = \mathbb{E}_{(Y(\cdot),X)\sim G}[\sum_{a=0}^mY(a)\pi(a|X)]
    = \mathbb{E}_{(Y(\cdot),X)\sim G, ~D \sim \pi_{b}}[\sum_{a=0}^m Y(a)D_{a}\frac{\pi(a|X)}{\pi_{b}(a|X)}]

where the last equality uses the independence of :math:`D` and :math:`Y(\cdot)` conditional on :math:`X` and the definition of :math:`\pi_b(\cdot|X)`.
We allow the counterfactual policy :math:`\pi` to be degenerate, i.e., it may choose a particular action with probability 1.
Estimating :math:`V^{\pi}` before implementing :math:`\pi` in an online environment is valuable because :math:`\pi` may perform poorly and damage user satisfaction.
Additionally, it is possible to select a counterfactual policy that maximizes the policy value by comparing their estimated performances.


Benchmark Estimators
-----------------------
There are several approaches to estimate the policy value of the counterfactual policy.

Direct Method (DM)
~~~~~~~~~~~~~~~~~~~~
A widely-used method, DM :cite:`Beygelzimer2009`, first learns a supervised machine learning model, such as random forest, ridge regression, and gradient boosting, to predict the mean reward function.
DM then uses it to estimate the policy value as

.. math::
    \hat{V}^{\pi}_{DM}=\frac{1}{T}\sum_{t=1}^T\sum_{a=0}^m\pi(a|X_t)\hat{\mu}(a|X_t).

where :math:`\hat{\mu}(a| x)` is the estimated reward function.
If :math:`\hat{\mu}(a| x)` is a good approximation to the mean reward function, this estimator accurately predicts the policy value of the counterfactual policy :math:`V^{\pi}`.
If :math:`\hat{\mu}(a| x)` fails to approximate the mean reward function well, however, the final estimator is no longer consistent.
The model misspecification issue is problematic because the extent of misspecification cannot be easily quantified from data :cite:`Farajtabar2018`.


Inverse Probability Weighting (IPW)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
To alleviate the issue with DM, researchers often use another estimator called IPW :cite:`Precup2000` :cite:`Strehl2010`.
IPW re-weights the rewards by the ratio of the counterfactual policy and behavior policy as

.. math::
    \hat{V}^{\pi}_{IPW}=\frac{1}{T}\sum_{t=1}^T\sum_{a=0}^m  Y_tD_{ta}\frac{\pi(a|X_t)}{\pi_b(a|X_t)}.

When the behavior policy is known, the IPW estimator is unbiased and consistent for the policy value.
However, it can have a large variance, especially when the counterfactual policy significantly deviates from the behavior policy.


Doubly Robust (DR)
~~~~~~~~~~~~~~~~~~~

The final approach is DR :cite:`Dudik2014`, which combines the above two estimators as

.. math::
    \hat{V}^{\pi}_{DR}=\frac{1}{T}\sum_{t=1}^T\sum_{a=0}^m \left\{ (Y_t-\hat{\mu}(a|X_t)) D_{ta} \frac{\pi(a|X_t)}{\pi_b(a|X_t)} + \pi(a|X_t)\hat{\mu}(a|X_t) \right\}.

DR mimics IPW to use a weighted version of rewards, but DR also uses the estimated mean reward function as a control variate to decrease the variance.
It preserves the consistency of IPW if either the importance weight or the mean reward estimator is accurate (a property called \textit{double robustness}).
Moreover, DR is *semiparametric efficient* :cite:`Narita2019` when the mean reward estimator is correctly specified.
On the other hand, when it is wrong, this estimator can have larger asymptotic mean-squared-error than IPW :cite:`Kallus2019` and perform poorly in practice :cite:`Kang2007`.

