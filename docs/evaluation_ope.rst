================================================
Evaluation of Off-Policy Evaluation
================================================
Here we describe an experimental protocol to evaluate OPE estimators and use it to compare a wide variety of existing estimators.

We can empirically evaluate OPE estimators' performances by using two sources of logged bandit feedback collected by two different policies :math:`\pi^{(he)}` (**h**ypothetical **e**valuation policy) and :math:`\pi^{(hb)}` (**h**ypothetical **b**ehavior policy).
We denote log data generated by :math:`\pi^{(he)}` and :math:`\pi^{(hb)}` as :math:`\mathcal{D}^{(he)} := \{ (x^{(he)}_t, a^{(he)}_t, r^{(he)}_t) \}_{t=1}^T` and :math:`\mathcal{D}^{(hb)} := \{ (x^{(hb)}_t, a^{(hb)}_t, r^{(hb)}_t) \}_{t=1}^T`, respectively.
By applying the following protocol to several different OPE estimators, we can compare their estimation performances:


1. Define the evaluation and test sets as:
    * in-sample case: :math:`\mathcal{D}_{\mathrm{ev}} := \mathcal{D}^{(hb)}_{1:T}`, :math:`\mathcal{D}_{\mathrm{te}} := \mathcal{D}^{(he)}_{1:T}`
    * out-sample case: :math:`\mathcal{D}_{\mathrm{ev}} := \mathcal{D}^{(hb)}_{1:\tilde{t}}`, :math:`\mathcal{D}_{\mathrm{te}} := \mathcal{D}^{(he)}_{\tilde{t}+1:T}`
    where :math:`\mathcal{D}_{a:b} := \{ (x_t,a_t,r_t) \}_{t=a}^b `.
2. Estimate the policy value of :math:`\pi^{(he)}` using :math:`\mathcal{D}_{\mathrm{ev}}` by an estimator :math:`\hat{V}`. We can represent an estimated policy value by :math:`\hat{V}` as :math:`\hat{V} (\pi^{(he)}; \mathcal{D}_{\mathrm{ev}})`.
3. Estimate :math:`V(\pi^{(he)})` by the *on-policy estimation* and regard it as the ground-truth as

    .. math::
        \mathrm{on}} (\pi^{(he)}; \mathcal{D}_{\mathrm{te}}) := \mathbb{E}_{\mathcal{D}_{\mathrm{te}}} [r^{(he)}_t].

4. Compare the off-policy estimate $\hat{V}(\pi^{(he)}; \mathcal{D}_{\mathrm{ev}})$ with its ground-truth $\mathrm{on}} (\pi^{(he)}; \mathcal{D}_{\mathrm{te}})$. We can evaluate the estimation accuracy of $\hat{V}$ by the following *relative estimation error* (relative-EE):

    .. math::
        relative-EE (\hat{V}; \mathcal{D}_{\mathrm{ev}}) := \left| \frac{\hat{V} (\pi^{(he)}; \mathcal{D}_{\mathrm{ev}}) - \mathrm{on}} (\pi^{(he)}; \mathcal{D}_{\mathrm{te}}) }{\mathrm{on}} (\pi^{(he)}; \mathcal{D}_{\mathrm{te}})} \right|.

5. To estimate standard deviation of relative-EE, repeat the above process several times with different bootstrap samples of the logged bandit data created by sampling data *with replacement* from :math:`\mathcal{D}_{\mathrm{ev}}`.

We call the problem setting **without** the sample splitting by time series as in-sample case.
In contrast, we call that **with** the sample splitting as out-sample case where OPE estimators aim to estimate the policy value of an evaluation policy in the test data.

The following algorithm describes the detailed experimental protocol to evaluate OPE estimators.

.. image:: ./_static/images/evaluation_of_ope_algo.png
   :scale: 25%
   :align: center
