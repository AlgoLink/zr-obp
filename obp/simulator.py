# Copyright (c) ZOZO Technologies, Inc. All rights reserved.
# Licensed under the Apache 2.0 License.

from dataclasses import dataclass
import logging
from tqdm import tqdm
from typing import List, Optional

import numpy as np
from sklearn.base import BaseEstimator

from obp.dataset import TRAIN_DICT
from obp.policy import BanditPolicyType
from obp.utils import check_is_fitted


@dataclass
class OfflineBanditSimulator:
    """A simulator class to run bandit algorithms on logged bandit feedback data.
    By using the log data generated by a counterfactual policy, this class also estimates the poicy value
    of a given counterfactual policy by off-poliy estimators such as DM, IPW and DR.
    Users are free to add their own estimators and use it to evaluate a new bandit policy.

    Parameters
    ----------
    train: TRAIN_DICT
        Training set of a logged bandit feedback data to be used in an offline bandit simulation.

    regression_model: BaseEstimator, default: None
        ML model to predict the mean reward function (E[R | X, A]).

    X_action: np.ndarray (n_actions, n_features_of_actions), default: None
        Context vectors used as an input to predict the mean reward function.

    is_output_proba: bool, default: True
        Whether the regression model outputs probabilities (classification) or not (regression).
    """
    train: TRAIN_DICT
    regression_model: Optional[BaseEstimator] = None
    X_action: Optional[np.ndarray] = None
    is_output_proba: bool = True

    def __post_init__(self) -> None:
        self._check_train_dict(train=self.train)
        if (self.regression_model is not None):
            if check_is_fitted(self.regression_model):
                logging.info("a fitted regression model is given.")
            else:
                logging.info("the given regression model is not fitted, and thus train it here...")
                X = self._pre_process_to_train_reg_model()
                reward = self.train['reward']
                self.regression_model.fit(X, reward)
        else:
            logging.warning(
                """regression model is not given, and thus model dependent estimators
                such as DM and DR cannot be used.""")

    def _pull(self,
              action: int,
              reward: float,
              position: int,
              pscore: float,
              context: np.ndarray) -> List[np.ndarray]:
        """Select a list of actions and update a policy.

        Parameters
        ----------
        action: int
            Action that was chosen for the log chosen by a behavior policy.

        reward: float
            Reward that was observed for the chosen action and position.

        position: int
            Position that the action was presented in the log.

        pscore: float
            Propensity score from a behavior policy that generated the log for the action chosen by it.

        context: np.ndarray, (1, dim_context)
            Context vector that characterizes the sample and would be used as an input for a counterfactual policy.
        """
        # select a list of actions
        if self.policy.policy_type == 'contextfree':
            selected_actions = self.policy.select_action()
        elif self.policy.policy_type == 'contextual':
            selected_actions = self.policy.select_action(context)
        action_match = action == selected_actions[position]
        # update parameters of a bandit policy
        # only when selected actions&positions are equal to logged actions&positions
        if action_match:
            if self.policy.policy_type == 'contextfree':
                self.policy.update_params(action=action, reward=reward)
            elif self.policy.policy_type == 'contextual':
                self.policy.update_params(action=action, reward=reward, context=context)

        return selected_actions

    def simulate(self, policy: BanditPolicyType) -> None:
        """Run bandit algorithms on a given training set.

        Parameters
        ----------
        policy: BanditPolicyType
            Bandit policy to be used in an offline simulation (i.e., counterfactual or evaluation policy).
        """
        self.policy = policy
        selected_actions_list = list()
        action, position, reward, pscore, context =\
            self.train['action'], self.train['position'], self.train['reward'],\
            self.train['pscore'], self.train['X_policy']
        data_size, dim_context = context.shape
        for action_, reward_, position_, pscore_, context_ in\
                tqdm(zip(action, reward, position, pscore, context), total=data_size):

            selected_actions = self._pull(
                action=action_,
                reward=reward_,
                position=position_,
                pscore=pscore_,
                context=context_.reshape(1, dim_context))
            selected_actions_list.append(selected_actions)

        self.selected_actions = np.array(selected_actions_list)
        if self.regression_model is not None:
            X = self._pre_process_to_predict_reward_function_with_reg_model()
            self.estimated_reward = self._estimate_reward_function(X=X)

    def replay_method(self) -> float:
        """Estimate the policy value by Relpay Method (RM).

        Note
        ----
        In order for this estimator to be unbiased, the actions on the logged samples
        must have been collected uniformly at random and not generated by some other policies.

        Returns
        -------
        V_hat: float
            Estimated policy value (performance) of a given counterfactual or evaluation policy.

        References
        ----------
        Lihong Li, Wei Chu, John Langford, and Xuanhui Wang.
        "Unbiased Offline Evaluation of Contextual-bandit-based News Article Recommendation Algorithms.", 2011.
        """
        n_data = self.train['n_data']
        reward = self.train['reward']
        action = self.train['action']
        position = self.train['position']
        action_match = action == self.selected_actions[np.arange(n_data), position]

        V_hat = 0.
        if np.sum(action_match) > 0.:
            V_hat = np.sum(action_match * reward) / np.sum(action_match)
        return V_hat

    def inverse_probability_weighting(self,
                                      min_pscore: float = 0.,
                                      is_self_normalized: bool = False) -> float:
        """Estimate the policy value by Inverse Probability Weighting (IPW).

        Note
        ----
        IPW re-weights the rewards by the ratio of the counterfactual policy and behavior policy.
        When the behavior policy is known, the IPW estimator is unbiased and consistent for the policy value.
        However, it can have a large variance, especially when the counterfactual policy significantly deviates from the behavior policy.

        Parameters
        ----------
        min_pscore: float, default: 0.
            Minimum value used as propensity score.
            Propensity scores larger than this parameter would be clipped.

        is_self_normalized: bool, default: False
            Whether the self-normalized technique is used.

        Returns
        -------
        V_hat: float
            Estimated policy value (performance) of a given counterfactual or evaluation policy.

        References
        ----------
        Alex Strehl, John Langford, Lihong Li, and Sham M Kakade.
        "Learning from Logged Implicit Exploration Data"., 2010.

        Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li.
        "Doubly Robust Policy Evaluation and Optimization.", 2014.

        Adith Swaminathan and Thorsten Joachims.
        "The Self-normalized Estimator for Counterfactual Learning.", 2015.
        """
        assert min_pscore <= 1., f'minimum propensity score must be lower than 1, but {min_pscore} is set.'

        n_data = self.train['n_data']
        reward = self.train['reward']
        action = self.train['action']
        position = self.train['position']
        pscore = np.clip(self.train['pscore'], a_min=min_pscore, a_max=1.0)
        action_match = action == self.selected_actions[np.arange(n_data), position]
        if is_self_normalized:
            return np.sum(action_match * reward / pscore) / np.sum(1. / pscore)
        else:
            return np.mean(action_match * reward / pscore)

    def direct_method(self) -> float:
        """Estimate the policy value by Direct Method (DM).

        Note
        ----
        This estimator first learns a supervised machine learning model, such as random
        forest, ridge regression, and gradient boosting, to predict the mean reward function (E[R | X, A]).
        It then uses it to estimate the policy value.

        If the regression model is a good approximation to the mean reward function,
        this estimator accurately predicts the policy value of the counterfactual policy.
        If the regression function fails to approximate the mean reward function well,
        however, the final estimator is no longer consistent.

        Parameters
        ----------
        min_pscore: float, default: 0.
            Minimum value used as propensity score.
            Propensity scores larger than this parameter would be clipped.

        Returns
        -------
        V_hat: float
            Estimated policy value (performance) of a given counterfactual or evaluation policy.

        References
        ----------
        Alina Beygelzimer and John Langford.
        "The offset tree for learning with partial labels.", 2009.

        Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li.
        "Doubly Robust Policy Evaluation and Optimization.", 2014.
        """
        assert self.regression_model is not None, 'a pre-trained regression model must be set.'

        return np.mean(self.estimated_reward)

    def doubly_robust(self, min_pscore: float = 0.,) -> float:
        """Estimate the policy value by Doubly Robust (DR).

        Note
        ----
        DR mimics IPW to use a weighted version of rewards, but DR also uses the estimated mean reward
        function (the regression model) as a control variate to decrease the variance.
        It preserves the consistency of IPW if either the importance weight or
        the mean reward estimator is accurate (a property called double robustness).
        Moreover, DR is semiparametric efficient [29] when the mean reward estimator is correctly specified.

        Returns
        -------
        V_hat: float
            Estimated policy value (performance) of a given counterfactual or evaluation policy.

        References
        ----------
        Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li.
        "Doubly Robust Policy Evaluation and Optimization.", 2014.
        """
        assert self.regression_model is not None, 'a pre-trained regression model must be set.'
        assert min_pscore <= 1., f'minimum propensity score must be lower than 1, but {min_pscore} is set.'

        n_data = self.train['n_data']
        reward = self.train['reward']
        action = self.train['action']
        position = self.train['position']
        pscore = np.clip(self.train['pscore'], a_min=min_pscore, a_max=1.0)
        action_match = action == self.selected_actions[np.arange(n_data), position]
        shifted_rewards = action_match * (reward - self.estimated_reward) / pscore
        return np.mean(shifted_rewards + self.estimated_reward)

    def _estimate_reward_function(self, X: np.ndarray) -> np.ndarray:
        """Predict the mean reward function by a given, pre-trained regression model."""
        if self.is_output_proba:
            return self.regression_model.predict_proba(X)[:, 1]
        else:
            return self.regression_model.predict(X)

    def _pre_process_to_train_reg_model(self) -> np.ndarray:
        """Preprocess feature vectors to train a give regression model.
        Please override this method if you want to use another feature enginnering for training the regression model.
        """
        return np.c_[self.train['position'], self.train['X_user'], self.X_action[self.train['action']]]

    def _pre_process_to_predict_reward_function_with_reg_model(self) -> np.ndarray:
        """Preprocess feature vectors to predict the mean reward function by a give regression model.
        Please override this method if you want to use another feature enginnering for training the regression model.
        """
        n_data = self.train['n_data']
        position = self.train['position']
        selected_actions_at_positions = self.selected_actions[np.arange(n_data), position]
        return np.c_[self.train['position'],
                     self.train['X_user'],
                     self.X_action[selected_actions_at_positions]]

    def _check_train_dict(self, train: TRAIN_DICT) -> RuntimeError:
        """Check keys of input train dict."""
        for key_ in ['action', 'position', 'reward', 'pscore', 'X_policy']:
            if key_ not in train:
                raise RuntimeError(f"Missing key of {key_} in 'train'")
