# Copyright (c) Yuta Saito, Yusuke Narita, and ZOZO Technologies, Inc. All rights reserved.
# Licensed under the Apache 2.0 License.

"""Class for Generating Synthetic Logged Bandit Feedback."""
from copy import deepcopy
from dataclasses import dataclass
from typing import Optional, Union, List, Callable

import numpy as np
from scipy.stats import truncnorm
from sklearn.utils import check_random_state, check_scalar

from .base import BaseSyntheticBanditDataset
from ..types import BanditFeedback


@dataclass
class SyntheticBanditDataset(BaseSyntheticBanditDataset):
    """Class for generating synthetic bandit dataset.

    Note
    -----
    By calling the `obtain_batch_bandit_feedback` method several times,
    we have different bandit samples with the same setting.
    This can be used to estimate confidence intervals of the performances of OPE estimators.

    If None is set as `base_policy_function`, behavior policy will be a uniform random policy :math:`\\pi_u`.
    Otherwise, behavior policies are defined as

        .. math::

            \\pi_b (a | x) := \\alpha_b \\cdot \\pi (a|x) + (1.0 - \\alpha_b) \\cdot \\pi_{u} (a|x)

        where :math:`\\pi (a|x)` is define by the `base_policy_function` argument and :math:`\\alpha_b` is set by a user.
        When a list is given as the `\\alpha_b` argument, the synthetic bandit dataset is generated by corresponding multiple behavior policies.

    Parameters
    -----------
    n_actions: int
        Number of actions.

    dim_context: int, default=1
        Number of dimensions of context vectors.

    reward_type: str, default='binary'
        Type of reward variable, must be either 'binary' or 'continuous'.
        When 'binary' is given, rewards are sampled from the Bernoulli distribution.
        When 'continuous' is given, rewards are sampled from the truncated Normal distribution with `scale=1`.

    reward_function: Callable[[np.ndarray, np.ndarray], np.ndarray]], default=None
        Function generating expected reward with context and action context vectors,
        i.e., :math:`\\mu: \\mathcal{X} \\times \\mathcal{A} \\rightarrow \\mathbb{R}`.
        If None is set, context **independent** expected reward for each action will be
        sampled from the uniform distribution automatically.

    base_policy_function: Callable[[np.ndarray, np.ndarray], np.ndarray], default=None
        Function generating probability distribution over action space,
        i.e., :math:`\\pi: \\mathcal{X} \\rightarrow \\Delta(\\mathcal{A})`.
        If None is set, context **independent** uniform distribution will be used (uniform random behavior policy).

    alpha_b: list, int or float, default=0.8
        List of ratios of a uniform random policy when constructing **behavior** policies.
        Must be in the [0, 1] interval.

    rho_b: list of int or float, default=None
        List of proportions of strata generated by each behavior policy.
        When None is given, unifrom strata sizes are applied.
        This argument is valid only under multiple behavior policies settings.

    random_state: int, default=None
        Controls the random seed in sampling synthetic bandit dataset.

    dataset_name: str, default='synthetic_bandit_dataset'
        Name of the dataset.

    Examples
    ----------

    .. code-block:: python

        >>> import numpy as np
        >>> from obp.dataset import (
                SyntheticBanditDataset,
                logistic_reward_function,
                linear_policy_function
            )

        # generate synthetic contextual bandit feedback with 10 actions.
        >>> dataset = SyntheticBanditDataset(
                n_actions=10,
                dim_context=5,
                reward_type="binary",
                reward_function=logistic_reward_function,
                base_policy_function=linear_policy_function,
                random_state=12345
            )
        >>> bandit_feedback = dataset.obtain_batch_bandit_feedback(n_rounds=100000)
        >>> bandit_feedback
        {
            'n_rounds': 100000,
            'n_actions': 10,
            'context': array([[-0.20470766,  0.47894334, -0.51943872, -0.5557303 ,  1.96578057],
                    [ 1.39340583,  0.09290788,  0.28174615,  0.76902257,  1.24643474],
                    [ 1.00718936, -1.29622111,  0.27499163,  0.22891288,  1.35291684],
                    ...,
                    [ 1.36946256,  0.58727761, -0.69296769, -0.27519988, -2.10289159],
                    [-0.27428715,  0.52635353,  1.02572168, -0.18486381,  0.72464834],
                    [-1.25579833, -1.42455203, -0.26361242,  0.27928604,  1.21015571]]),
            'action_context': array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]),
            'action': array([5, 1, 1, ..., 8, 7, 7]),
            'position': array([0, 0, 0, ..., 0, 0, 0]),
            'reward': array([0, 1, 1, ..., 1, 0, 1]),
            'expected_reward': array([[0.99170781, 0.86788988, 0.82500399, ..., 0.61877815, 0.47923429,
                    0.91073805],
                    [0.84626883, 0.98812369, 0.9612451 , ..., 0.57832262, 0.0046887 ,
                    0.81732406],
                    [0.83649314, 0.96980762, 0.9904457 , ..., 0.92180661, 0.02244647,
                    0.69689282],
                    ...,
                    [0.02791925, 0.30607386, 0.07639342, ..., 0.39065817, 0.81672022,
                    0.07918226],
                    [0.79779238, 0.72380977, 0.5621137 , ..., 0.44101395, 0.25944593,
                    0.40214268],
                    [0.88135157, 0.50334267, 0.90011717, ..., 0.76787883, 0.928192  ,
                    0.59051038]]),
            'pscore': array([0.04571134, 0.50202154, 0.134954  , ..., 0.25303061, 0.02460262,
                    0.06952335])
        }

    """

    n_actions: int
    dim_context: int = 1
    reward_type: str = "binary"
    reward_function: Optional[Callable[[np.ndarray, np.ndarray], np.ndarray]] = None
    base_policy_function: Optional[
        Callable[[np.ndarray, np.ndarray], np.ndarray]
    ] = None
    alpha_b: Union[List[Union[int, float]], int, float] = 0.8
    rho_b: Optional[List[Union[int, float]]] = None
    random_state: Optional[int] = None
    dataset_name: str = "synthetic_bandit_dataset"

    def __post_init__(self) -> None:
        """Initialize Class."""
        assert self.n_actions > 1 and isinstance(
            self.n_actions, int
        ), f"n_actions must be an integer larger than 1, but {self.n_actions} is given"
        assert self.dim_context > 0 and isinstance(
            self.dim_context, int
        ), f"dim_context must be a positive integer, but {self.dim_context} is given"
        assert self.reward_type in [
            "binary",
            "continuous",
        ], f"reward_type must be either 'binary' or 'continuous', but {self.reward_type} is given"

        if isinstance(self.alpha_b, list):
            for alpha_b_ in self.alpha_b:
                check_scalar(
                    alpha_b_,
                    name="an element of alpha_b",
                    target_type=(int, float),
                    min_val=0.0,
                    max_val=1.0,
                )
            if self.rho_b is None:
                self.rho_b_ = (
                    np.ones(self.n_behavior_policies) / self.n_behavior_policies
                )
            else:
                assert isinstance(
                    self.rho_b, list
                ), "when alpha_b is a list, rho_b must be a list"
                assert len(self.alpha_b) == len(
                    self.rho_b
                ), "alpha_b and rho_b must have the same lengths"
                for rho_b_ in self.rho_b:
                    check_scalar(
                        rho_b_,
                        name="an element of rho_b",
                        target_type=(int, float),
                        min_val=0.0,
                    )
                self.rho_b_ = np.array(self.rho_b) / np.sum(self.rho_b)
        else:
            check_scalar(
                self.alpha_b,
                name="alpha_b",
                target_type=(int, float),
                min_val=0.0,
                max_val=1.0,
            )
        self.alpha_b_ = deepcopy(self.alpha_b)

        self.random_ = check_random_state(self.random_state)
        if self.reward_function is None:
            self.expected_reward = self.sample_contextfree_expected_reward()
        if self.base_policy_function is None:
            self.pi_b = np.ones(self.n_actions) / self.n_actions
        # one-hot encoding representations characterizing each action
        self.action_context = np.eye(self.n_actions, dtype=int)

    @property
    def len_list(self) -> int:
        """Length of recommendation lists."""
        return 1

    @property
    def n_behavior_policies(self) -> int:
        """Number of behavior policies."""
        if isinstance(self.alpha_b, float):
            return 1
        else:
            return len(self.alpha_b)

    def sample_contextfree_expected_reward(self) -> np.ndarray:
        """Sample expected reward for each action from the uniform distribution."""
        return self.random_.uniform(size=self.n_actions)

    def obtain_batch_bandit_feedback(self, n_rounds: int) -> BanditFeedback:
        """Obtain batch logged bandit feedback.

        Parameters
        ----------
        n_rounds: int
            Number of rounds for synthetic bandit feedback data.

        Returns
        ---------
        bandit_feedback: BanditFeedback
            Generated synthetic bandit feedback dataset.

        """
        assert n_rounds > 0 and isinstance(
            n_rounds, int
        ), f"n_rounds must be a positive integer, but {n_rounds} is given"

        # sample context vectors from the standard normal distribution
        context = self.random_.normal(size=(n_rounds, self.dim_context))
        # construct behavior policies
        if self.base_policy_function is None:
            base_pi_b = np.tile(self.pi_b, (n_rounds, 1))
        else:
            base_pi_b = self.base_policy_function(
                context=context,
                action_context=self.action_context,
                random_state=self.random_state,
            )
        # single behavior policy case
        if self.n_behavior_policies == 1:
            pi_b = np.zeros((n_rounds, self.n_actions))
            pi_b[:, :] = (
                self.alpha_b * base_pi_b + (1.0 - self.alpha_b) / self.n_actions
            )
        # multiple behavior (logging) policies case
        else:
            # calculate strata sizes based on rho_b
            n_strata = (self.rho_b_ * n_rounds).astype(int)
            n_strata[-1] += n_rounds - n_strata.sum()
            strata_idx_list = list()
            for stratum_id, n_stratum in enumerate(n_strata):
                strata_idx_list += [stratum_id] * n_stratum
            self.strata_idx = np.array(strata_idx_list)
            self.rho_b_ = n_strata / n_strata.sum()
            # construct multiple behavior (logging) policies and their marginal one
            pi_b_dict = dict()
            pi_b = np.zeros((n_rounds, self.n_actions))
            pi_b_star = np.zeros((n_rounds, self.n_actions))
            for pi_b_id, alpha_b_ in enumerate(self.alpha_b_):
                pi_b_ = np.zeros((n_rounds, self.n_actions))
                pi_b_[:, :] = alpha_b_ * base_pi_b + (1.0 - alpha_b_) / self.n_actions
                pi_b_dict[pi_b_id] = pi_b_
                pi_b_star += self.rho_b_[pi_b_id] * pi_b_
                pi_b[self.strata_idx == pi_b_id] = pi_b_[self.strata_idx == pi_b_id]

        # sample actions for each round based on the behavior policy
        action = np.array(
            [
                self.random_.choice(np.arange(self.n_actions), p=pi_b[i],)
                for i in np.arange(n_rounds)
            ]
        )
        # sample rewards for each round based on the reward function
        if self.reward_function is None:
            expected_reward_ = np.tile(self.expected_reward, (n_rounds, 1))
        else:
            expected_reward_ = self.reward_function(
                context=context,
                action_context=self.action_context,
                random_state=self.random_state,
            )
        expected_reward_factual = expected_reward_[np.arange(n_rounds), action]
        if self.reward_type == "binary":
            reward = self.random_.binomial(n=1, p=expected_reward_factual)
        elif self.reward_type == "continuous":
            min_, max_ = 0, 1e10
            mean, std = expected_reward_factual, 1.0
            a, b = (min_ - mean) / std, (max_ - mean) / std
            reward = truncnorm.rvs(
                a=a, b=b, loc=mean, scale=std, random_state=self.random_state
            )
            # correct expected_reward_, as we use truncated normal distribution here
            mean = expected_reward_
            a, b = (min_ - mean) / std, (max_ - mean) / std
            expected_reward_ = truncnorm.stats(
                a=a, b=b, loc=mean, scale=std, moments="m"
            )

        if self.n_behavior_policies == 1:
            return dict(
                n_rounds=n_rounds,
                n_actions=self.n_actions,
                context=context,
                action_context=self.action_context,
                action=action,
                position=np.zeros(n_rounds, dtype=int),
                reward=reward,
                expected_reward=expected_reward_,
                pscore=pi_b[np.arange(n_rounds), action],
            )
        else:
            return dict(
                n_rounds=n_rounds,
                n_actions=self.n_actions,
                n_behavior_policies=self.n_behavior_policies,
                rho_b=self.rho_b_,
                strata_idx=self.strata_idx,
                context=context,
                action_context=self.action_context,
                action=action,
                reward=reward,
                position=np.zeros(n_rounds, dtype=int),
                expected_reward=expected_reward_,
                pscore=pi_b[np.arange(n_rounds), action],
                marginal_pscore=pi_b_star[np.arange(n_rounds), action],
            )
